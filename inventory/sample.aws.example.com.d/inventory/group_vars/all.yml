---

# 'hosting_infrastructure' is used to drive the correct behavior based
# on the hosting infrastructure, cloud provider, etc. Valid values are:
# - 'openstack'
# - 'aws'
# - 'azure' (Coming Soon)
hosting_infrastructure: aws

## AWS Provisioning Variables
# Amazon image name to use for instances.
# Note that each AWS region may utilize a different AMI for the same image.
aws_image_name: <REPLACE WITH VALID AMI>

# AWS instance flavors (i.e.: instance sizes)
master_flavor: m4.xlarge
infra_flavor: i3.xlarge
node_flavor: m4.xlarge

# Number of master instances to deploy.
# This is a non-HA sample nventory so don't change this value
aws_num_masters: 1

# Number of instances to be used as compute/app nodes
aws_num_nodes: 3

# Number of instances to be used as infra nodes.
# This is a non-HA sample inventory so don't change this value
aws_num_infra: 1

# Cluster Environment ID to uniquely identify the environment
env_id: "<REPLACE WITH VALID ENV ID - i.e: env1>"

# DNS configurations
# the 'public_dns_domain' will be used as the base domain for the deployment
# the 'public_dns_nameservers' is a list of DNS resolvers the cluster should use
public_dns_domain: "<REPLACE WITH A VALID DNS DOMAIN>"
public_dns_nameservers:
- 8.8.8.8

# root (/) volume size for the instances
master_root_volume_size: 50
infra_root_volume_size: 50
node_root_volume_size: 50
gluster_root_volume_size: 50

# Docker Storage configuration variables
docker_volume_size: "40"
docker_storage_block_device: "/dev/xvdb"

# AWS access keys used by the tools to interact with ec2
# This example uses ENV variables (recommended), but the values
# can also be specified here
aws_access_key: "{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
aws_secret_key: "{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"

aws_region: <REPLACE WITH VALID AWS REGION>

# If aws_create_vpc is 'true', aws_subnet variable will be ignored
aws_create_vpc: true
aws_subnet: <REPLACE WITH VALID SUBNET ID>

# use the "-e" option to set "aws_key_name"
#aws_key_name: my-ssh-region-key


# These are the security groups created when aws_create_vpc is 'true'. Modify accordingly to your environment in case using existing VPC and SGs
aws_master_sgroups: ['ocp-ssh', 'ocp-master', 'ocp-node']
aws_infra_sgroups: ['ocp-ssh', 'ocp-infra', 'ocp-node']
aws_node_sgroups: ['ocp-ssh', 'ocp-node']

# Tags used to identify the instances
# Note: Be careful when changing these as the tools rely on some of these values
# to be set correctly.
master_tag: master
etcd_tag: etcd
infranode_tag: infranode
computenode_tag: computenode
role_tag: ocp
user_tag: <REPLACE WITH A TAG DESCRIBING THE USER OWNING THE CLUSTER>
group_masters_tag: masters_aws
group_infra_tag: infra_aws
group_compute_tag: nodes_aws
labels_master_tag: '{"region": "default"}'
labels_infra_tag: '{"region": "infra"}'
labels_compute_tag: '{"region": "primary"}'

# Tags used to identify the CNS instances
# Note: Be careful when changing these as the tools rely on some of these values
# to be set correctly.
cns_deploy: false
gluster_flavor: i3.xlarge
gluster_tag: cns
group_gluster_tag: cns_aws
gluster_volume_size: "200"
gluster_storage_block_device: "/dev/xvdg"
aws_gluster_sgroups: ['ocp-ssh', 'ocp-node', 'ocp-cns']
labels_cns_tag: '{"region": "primary"}'

# Should the Instance Termination Protection be set yes/no
instances_termination_protection: yes

# Should the cluster be configured for HA
# - currently not supported
ha_mode: false

# Subscription Management Details
rhsm_register: True
rhsm_repos:
 - "rhel-7-server-rpms"
 - "rhel-7-server-ose-3.6-rpms"
 - "rhel-7-server-extras-rpms"
 - "rhel-7-fast-datapath-rpms"
 - "rhel-server-rhscl-7-rpms"


# Use RHSM username, password and optionally pool:
# NOTE: use the -e option to specify on CLI instead of statically set here
rhsm_username: '<REPLACE WITH VALID RHSM USERNAME>'
rhsm_password: '<REPLACE WITH VALID RHSM PASSWORD>'

# leave commented out if you want to `--auto-attach` a pool
#rhsm_pool: ''
